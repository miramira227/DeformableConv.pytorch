{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deformable Conv의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMlpobSv/Moiu1X8h6lpyok",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "db6b010f4c6b4edf96db2fac9bda7246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a12a6f7bd5ea4cedb99bc183f00b620a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0342f336e207407590262eda9bd72382",
              "IPY_MODEL_769f041f550040cb99b9a931b13d9532"
            ]
          }
        },
        "a12a6f7bd5ea4cedb99bc183f00b620a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0342f336e207407590262eda9bd72382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e9f1a7c426864016a8afd3d2fc7fdae7",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 178728960,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 178728960,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8cad9065905d4e00b564b22f548c6e86"
          }
        },
        "769f041f550040cb99b9a931b13d9532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e994ac29405640509a7a13007c13f4cc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170M/170M [00:07&lt;00:00, 24.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_315492b76d3d4a489b159b8273458724"
          }
        },
        "e9f1a7c426864016a8afd3d2fc7fdae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8cad9065905d4e00b564b22f548c6e86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e994ac29405640509a7a13007c13f4cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "315492b76d3d4a489b159b8273458724": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miramira227/DeformableConv1_pytorch/blob/master/Deformable_Conv%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV6eGWi9WL4k",
        "colab_type": "text"
      },
      "source": [
        "##**0. Device Setting**##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McrkXTKFWPlF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ebfe3853-07d9-421a-8548-d3d254006101"
      },
      "source": [
        "import torch \n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "dtype = torch.float\n",
        "\n",
        "print(f'torch device is {device}')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch device is cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYlvMM6jHLSk",
        "colab_type": "text"
      },
      "source": [
        "##**1. Resize Data Shape**##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zSsNTebGqnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import os \n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np \n",
        "import json \n",
        "\n",
        "def get_input_img(idx, mode):\n",
        "  \n",
        "  if mode != 'test':\n",
        "    json_path = f'/content/data/annotations/instances_{mode}2017.json'\n",
        "    \n",
        "    with open(json_path) as f:\n",
        "      json_data = json.load(f)\n",
        "  \n",
        "  path = f'/content/data/{mode}2017/'\n",
        "\n",
        "\n",
        "  file_list = os.listdir(path)\n",
        "\n",
        "  while idx : \n",
        "    img_jpg = file_list_jpg[idx]\n",
        "  \n",
        "    if folder != 'test':\n",
        "      for image in json_data['images']:\n",
        "        if image['file_name'] == img_jpg:      # img = '000000368456.jpg'\n",
        "          img_h = image['height']\n",
        "          img_w = image['width']\n",
        "          img_id = image['id']\n",
        "          break\n",
        "\n",
        "      for anno in json_data['annotations']:\n",
        "        if anno['image_id'] == img_id:\n",
        "          img_gt_bboxes = anno['bbox']\n",
        "          img_class = anno['category_id']     # 총 90개의 카테고리, 33: suitcase 7: train \n",
        "          break\n",
        "      \n",
        "      if (anno['image_id'] != img_id) or (image['file_name'] != img_jpg) :\n",
        "        idx += 1\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    print(f'{idx}번째 이미지')\n",
        "\n",
        "    image = {}\n",
        "    image['img_h'] = img_h\n",
        "    image['img_w'] = img_w\n",
        "    image['img_id'] = img_id\n",
        "    image['img_gt_bboxes'] = img_gt_bboxes\n",
        "    image['img_class'] = img_class    # 38: kite \n",
        "  \n",
        "  img_path = path + img_jpg\n",
        "  img = cv2.imread(img_path)\n",
        "\n",
        "  # bounding box와 class 표시\n",
        "  image['img_gt_bboxes'] = [int(np.round(i)) for i in image['img_gt_bboxes']]\n",
        "  cv2.rectangle(img, (image['img_gt_bboxes'][0], image['img_gt_bboxes'][1]), (image['img_gt_bboxes'][0] + image['img_gt_bboxes'][2], image['img_gt_bboxes'][1] + image['img_gt_bboxes'][3]), (0, 0, 255), 2)\n",
        "  cv2.putText(img, str(image['img_class']), (image['img_gt_bboxes'][0], image['img_gt_bboxes'][1]), 1, 1, (255, 0, 0))\n",
        "  cv2_imshow(img)    \n",
        "\n",
        "  # ResNet의 input은 224 x 224로 규정 \n",
        "  W = 224\n",
        "  H = 224 \n",
        "\n",
        "  if folder == 'test':\n",
        "    image = {}\n",
        "    image['img_h'] = H\n",
        "    image['img_w'] = W\n",
        "\n",
        "  img = cv2.resize(img, (W, H), interpolation = cv2.INTER_AREA)\n",
        "  img = img.reshape(1, CH, H, W)\n",
        "  img = torch.tensor(img, dtype=dtype, device=device)\n",
        "\n",
        "  return image, img "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHaiwppaHTNJ",
        "colab_type": "text"
      },
      "source": [
        "##**1. Deformable Conv2d**##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IuNBgeCQjZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "import torch \n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import torch.optim as optim\n",
        "\n",
        "class DeformConv2d(nn.Module):\n",
        "\n",
        "  def __init__(self, D_in, D_out, k_size, stride = 1, padding = 0, dilation=1):    # D_in, D_out = 512, k_size = 3\n",
        "    super(DeformConv2d, self).__init__()\n",
        "    self.D_in = D_in          # input channel(512)\n",
        "    self.D_out = D_out        # output channel(512)\n",
        "    self.k_size = k_size      # kernel size = 3\n",
        "    self.padding = padding\n",
        "    self.stride = stride\n",
        "    self.dilation = dilation\n",
        "    self.off_conv = nn.Conv2d(self.D_in, 18, 3, padding=1)\n",
        "    self.deform_conv = nn.Conv2d(self.D_in, self.D_out, 3)\n",
        "    # self.bias = bias\n",
        "\n",
        "  def bilinear(self, h, w, float_h, float_w):       # 2d\n",
        "    return max(0, 1-abs(float_w - w)) * max(0, 1-abs(float_h - h))\n",
        "\n",
        "  def bilinear_value(self, float_h, float_w, padded_img):            # 3d    # padded_img = (512, 9, 9)\n",
        "    h_int = torch.tensor([math.floor(float_h), math.ceil(float_h)], dtype=torch.long, device=device).clamp(min=0, max=padded_img.size(2)-1)\n",
        "    w_int = torch.tensor([math.floor(float_w), math.ceil(float_w)], dtype=torch.long, device=device).clamp(min=0, max=padded_img.size(2)-1)\n",
        "\n",
        "    value = torch.zeros((padded_img.size(0)), dtype=dtype, device=device)   # (512)   -> 1-dim\n",
        "\n",
        "    for h in h_int:\n",
        "      for w in w_int:\n",
        "        value += padded_img[:, h, w] * self.bilinear(h, w, float_h, float_w)\n",
        "    return value # (512, 1, 1)\n",
        "\n",
        "  def deformable_grid(self, i, j, off_field, padded_img):            # 3d \n",
        "    batch = padded_img.size(0)\n",
        "\n",
        "    grid = torch.zeros((batch, self.D_out,) + (self.k_size,) * 2, dtype=dtype, device=device)    # (512, 3, 3)\n",
        "    h = [j + self.dilation * k for k in range(self.k_size)]     # j, j+1, j+2\n",
        "    w = [i + self.dilation * k for k in range(self.k_size)]     # i, i+1, i+2\n",
        "    \n",
        "    offset = off_field[:, :, j, i].view(batch, 2, -1)   # off_field = [batch, 18, 7, 7]    # [batch, 18]\n",
        "\n",
        "    for b in range(batch):\n",
        "      idx = 0\n",
        "      for ih, hei in enumerate(h):\n",
        "        for iw, wid in enumerate(w):\n",
        "          grid[b, :, ih, iw] = self.bilinear_value(hei+offset[b, 0, idx], wid+offset[b, 1, idx], padded_img[b])\n",
        "          idx += 1\n",
        "    return grid       # grid = [512, 3, 3]\n",
        "\n",
        "  def deformable_conv(self,in_img,padded_img, off_field):   \n",
        "    out_h = int((in_img.size(2) + self.padding * 2 - self.k_size) / (self.stride * self.dilation)) + 1\n",
        "    out_w = int((in_img.size(3) + self.padding * 2 - self.k_size) / (self.stride * self.dilation)) + 1\n",
        "\n",
        "    out_img = torch.zeros(in_img.size()[:2] + (out_h, out_w), dtype = dtype, device = device)\n",
        "\n",
        "    i = 0\n",
        "    for h in range(out_img.size(2)):    # 7\n",
        "      for w in range(out_img.size(3)):    # 7\n",
        "        out_img[:, :, h, w] = self.deform_conv(self.deformable_grid(h, w, off_field, padded_img)).squeeze()\n",
        "    return out_img\n",
        "\n",
        "  def forward(self, x, bias=None):   # x = (batch, 512, 7, 7)\n",
        "    off_field = self.off_conv(x)   # (batch, 18, 7, 7)\n",
        "    padded_img = F.pad(x, (1, ) * 4)\n",
        "    output_img = self.deformable_conv(x, padded_img, off_field)\n",
        "\n",
        "    return output_img\n",
        "  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7F_8GHXHeDb",
        "colab_type": "text"
      },
      "source": [
        "##**2. ResNet101 1st~4th block**##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c1ckYN7Qp8m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107,
          "referenced_widgets": [
            "db6b010f4c6b4edf96db2fac9bda7246",
            "a12a6f7bd5ea4cedb99bc183f00b620a",
            "0342f336e207407590262eda9bd72382",
            "769f041f550040cb99b9a931b13d9532",
            "e9f1a7c426864016a8afd3d2fc7fdae7",
            "8cad9065905d4e00b564b22f548c6e86",
            "e994ac29405640509a7a13007c13f4cc",
            "315492b76d3d4a489b159b8273458724"
          ]
        },
        "outputId": "5895db7a-317e-4a0e-8085-d9402feb4a37"
      },
      "source": [
        "import torchvision.models as models \n",
        "\n",
        "model = models.resnet101(pretrained=True, progress=True)\n",
        "removed = list(model.children())[:-3]\n",
        "model = torch.nn.Sequential(*removed)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db6b010f4c6b4edf96db2fac9bda7246",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=178728960.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLUAoUZXHgoR",
        "colab_type": "text"
      },
      "source": [
        "##**3. ResNet101 5th block**##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B035Y3OLQtjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.models as models\n",
        "from torchvision.models.resnet import Bottleneck\n",
        "import time\n",
        "import torch.nn as nn \n",
        "\n",
        "class MyConv5ofResnet101(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyConv5ofResnet101, self).__init__()\n",
        "    self.conv_deform_d1 = DeformConv2d(512, 512, 3, padding=1, dilation=1)\n",
        "    self.conv_deform_d2 = DeformConv2d(512, 512, 3, padding=1, dilation=2)\n",
        "\n",
        "    self.conv_begin = nn.Conv2d(1024, 512, 1, bias=False)\n",
        "    self.conv_no_begin = nn.Conv2d(2048, 512, 1, bias=False)\n",
        "    self.conv3rd = nn.Conv2d(512, 2048, 1, bias=False)\n",
        "    self.conv_downsample = nn.Conv2d(1024, 2048, 1, stride = 2, bias=False)\n",
        "    self.bn_downsample = nn.BatchNorm2d(2048)\n",
        "    self.bn = nn.BatchNorm2d(512)\n",
        "    self.relu = nn.ReLU(inplace=False)\n",
        "    self.conv_last = nn.Conv2d(2048, 1024, 1, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    input = x\n",
        "\n",
        "    for i in range(3):\n",
        "      if i == 0:\n",
        "        x = self.conv_begin(x)\n",
        "      else:\n",
        "        x = self.conv_no_begin(x)\n",
        "\n",
        "      x = self.bn(x)\n",
        "      if i == 0:\n",
        "        x = self.conv_deform_d2(x)\n",
        "      else:\n",
        "        x = self.conv_deform_d1(x)\n",
        "\n",
        "      x = self.bn(x)\n",
        "      x = self.conv3rd(x)\n",
        "      x = self.bn_downsample(x)\n",
        "      x = self.relu(x)\n",
        "\n",
        "      if i == 0:\n",
        "        x += self.conv_downsample(input)\n",
        "        x = self.bn_downsample(x)\n",
        "    x = self.conv_last(x)\n",
        "\n",
        "    return x       "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvVTKZXFHmq2",
        "colab_type": "text"
      },
      "source": [
        "##**4. RPN**##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVT-G2UjGSXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyRPN(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MyRPN, self).__init__()\n",
        "    # self.roi = DeformableRoI(7)\n",
        "    self.conv1 = nn.Conv2d(1024, 256, 3, padding=1)\n",
        "    self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    self.cls_layer = nn.Conv2d(256, 18, 1)      # for objectness  \n",
        "    self.reg_layer = nn.Conv2d(256, 36, 1)\n",
        "\n",
        "    self.softmax = nn.Softmax(dim = 2)\n",
        "\n",
        "  def forward(self, x):    # x.size() = [batch, 1024, 7, 7]\n",
        "    x = self.conv1(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    cls = self.cls_layer(x)\n",
        "    cls_size = cls.size()\n",
        "    cls = cls.view(cls.size(0), 9, 2, cls.size(2), cls.size(3))\n",
        "    cls = self.softmax(cls)\n",
        "    cls = cls.view(cls_size)\n",
        "\n",
        "    reg = self.reg_layer(x)\n",
        "    return cls, reg     # cls = [batch, 18, 7, 7], reg = [batch, 36, 7, 7]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjSZezN0Hv4r",
        "colab_type": "text"
      },
      "source": [
        "##**5. Make Anchor Boxes**##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zce4WgsVH0h5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 0. anchor scale and ratios\n",
        "def make_anchor_boxes(scales, ratios, image, cls):\n",
        "  scales_list = scales.repeat(len(ratios))      # tensor([128, 256, 512, 128, 256, 512, 128, 256, 512])\n",
        "  ratios_list = ratios.view(-1, 1).repeat(1, len(scales)).view(-1)   # tensor([0.5000, 0.5000, 0.5000, 1.0000, 1.0000, 1.0000, 2.0000, 2.0000, 2.0000])\n",
        "\n",
        "  img_h = image['img_h']      # 234\n",
        "  img_w = image['img_w']      # 500\n",
        "  img_id = image['img_id']\n",
        "  img_gt_bboxes = image['img_gt_bboxes']     # [18.17, 155.82, 137.1, 60.01]\n",
        "  img_class = image['img_class']        # 234 500\n",
        "\n",
        "  # 1. stride \n",
        "  stride_h = int(img_h / cls.size(2))   \n",
        "  stride_w = int(img_w / cls.size(3))\n",
        "  # print((stride_h), int(stride_w))    # 33 71\n",
        "\n",
        "\n",
        "  # 2. make base_anchor     # stride의 중간에서 시작\n",
        "  start_point = torch.tensor((int(stride_h / 2), int(stride_w / 2)), dtype = dtype, device = device)    # (16, 35)  # h, w 순서 \n",
        "  # print(start_point)      # tensor([16, 35])\n",
        "\n",
        "  start_h = torch.tensor(int(stride_h / 2), dtype = dtype, device = device)\n",
        "  start_w = torch.tensor(int(stride_w / 2), dtype = dtype, device = device)\n",
        "\n",
        "  grid_h = torch.arange(start_h, img_h, stride_h, dtype = dtype, device = device)\n",
        "  grid_w = torch.arange(start_w, img_w, stride_w, dtype = dtype, device = device)\n",
        "  # print(grid_h)   # tensor([ 16.,  49.,  82., 115., 148., 181., 214.])\n",
        "  # print(grid_w)  # tensor([ 35., 106., 177., 248., 319., 390., 461.])\n",
        "\n",
        "  grid_hh = grid_h.view(-1, 1).repeat(1, len(grid_w))\n",
        "  grid_ww = grid_w.repeat(len(grid_h), 1)\n",
        "  grid_hh_flat = grid_hh.view(-1, 1)\n",
        "  grid_ww_flat = grid_ww.view(-1, 1)\n",
        "\n",
        "  grid = torch.stack([grid_ww_flat, grid_hh_flat, grid_ww_flat, grid_hh_flat], dim=-1)     # torch.Size([49, 1, 4])\n",
        "\n",
        "  grid = grid.repeat(1, 9, 1).view(-1, 4)   # torch.Size([441, 4])  \n",
        "  # anchor for regression\n",
        "\n",
        "  anchor_w = scales_list * ratios_list    # tensor([  64.,  128.,  256.,  128.,  256.,  512.,  256.,  512., 1024.])\n",
        "  anchor_h = anchor_w / ratios_list   # tensor([128., 256., 512., 128., 256., 512., 128., 256., 512.])\n",
        "  anchor_w = anchor_w.view(-1, 1).repeat((grid.size(0) // anchor_w.size(0), 1))    # torch.Size([441, 1])\n",
        "  anchor_h = anchor_h.view(-1, 1).repeat((grid.size(0) // anchor_h.size(0), 1))     # torch.Size([441, 1])\n",
        "\n",
        "  anchor = torch.stack([(anchor_w // -2), (anchor_h // -2), anchor_w // 2, anchor_h // 2], dim = -1).squeeze()    # torch.Size([441, 4])\n",
        "\n",
        "  anchor_xyxy = grid + anchor    # total anchor boxes in xyxy format , 7 x 7 x 9 x 4\n",
        "\n",
        "  # anchor for loss calculation\n",
        "  anchor_grid_x1 = anchor_xyxy[:, 0]\n",
        "  anchor_grid_y1 = anchor_xyxy[:, 1]\n",
        "  anchor_grid_x2 = anchor_xyxy[:, 2]\n",
        "  anchor_grid_y2 = anchor_xyxy[:, 3]\n",
        "\n",
        "  anchor_x_ctr = (anchor_grid_x1 + anchor_grid_x2) // 2\n",
        "  anchor_y_ctr = (anchor_grid_y1 + anchor_grid_y2) // 2\n",
        "  anchor_w = (anchor_grid_x2 - anchor_grid_x1)\n",
        "  anchor_h = (anchor_grid_y2 - anchor_grid_y1)\n",
        "\n",
        "  anchor_xywh = torch.stack([anchor_x_ctr, anchor_y_ctr, anchor_w, anchor_h], dim = -1)   # total anchor boxes in xctr-yctr-w-h format \n",
        "  return anchor_xyxy, anchor_xywh"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z08dHpqWkODM",
        "colab_type": "text"
      },
      "source": [
        "##**6. Get IoU, regressed_gt box**##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHUZxn1780ZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_gt_xyxy(gt_bboxes):\n",
        "  gt_box = gt_bboxes\n",
        "  gt_xyxy = torch.tensor([gt_box[0], gt_box[1], gt_box[0]+gt_box[2], gt_box[1] + gt_box[3]]) \n",
        "  return gt_xyxy\n",
        "\n",
        "\n",
        "def get_IoU(gt_xyxy, anchor_xyxy):\n",
        "  gt_area = (gt_xyxy[2]-gt_xyxy[0]) * (gt_xyxy[3] - gt_xyxy[1])\n",
        "  anchor_area = (anchor_xyxy[:, 2]-anchor_xyxy[:, 0]) * (anchor_xyxy[:, 3] - anchor_xyxy[:, 1])\n",
        "\n",
        "  inter_area = torch.zeros_like(anchor_area)\n",
        "\n",
        "  cond1 = torch.min(anchor_xyxy[:, 2], gt_xyxy[2].float()) - torch.max(anchor_xyxy[:, 0], gt_xyxy[0].float()) \n",
        "  cond2 = torch.min(anchor_xyxy[:, 3], gt_xyxy[3].float()) - torch.max(anchor_xyxy[:, 1], gt_xyxy[1].float()) \n",
        "\n",
        "  inter_area = cond1 * cond2\n",
        "  inter_area = inter_area.where((cond1 >= 0) & (cond2 >= 0), torch.tensor(0).float())   # 아닐 때의 값   \n",
        "\n",
        "  IoU = inter_area / (gt_area + anchor_area - inter_area)  # IoU for each anchor with gt box    # torch.Size([441])\n",
        "  return IoU\n",
        "\n",
        "\n",
        "def make_label(IoU): \n",
        "  # 1. default = zero(ignore)\n",
        "  label = torch.zeros_like(IoU)\n",
        "\n",
        "  # 2. negative \n",
        "  label[IoU < 0.3] = -1\n",
        "\n",
        "  # 3. positive > 0.7\n",
        "  label[IoU > 0.7] = 1\n",
        "\n",
        "  # 4. max > 0.7\n",
        "  label[torch.argmax(IoU)] = 1\n",
        "  return label \n",
        "\n",
        "\n",
        "# regress \n",
        "def get_gt_reg(gt_xyxy, anchor_xywh):\n",
        "  gt_x_ctr = (gt_xyxy[0] + gt_xyxy[2]) // 2\n",
        "  gt_y_ctr = (gt_xyxy[1] + gt_xyxy[3]) // 2\n",
        "  gt_w = gt_xyxy[2] - gt_xyxy[0]\n",
        "  gt_h = gt_xyxy[3] - gt_xyxy[1]\n",
        "\n",
        "  pos_labeled_anchor = anchor_xywh[label == 1]\n",
        "\n",
        "  if len(pos_labeled_anchor) > 1:\n",
        "    [pos_x_ctr,pos_y_ctr, pos_w, pos_h] = [pos_labeled_anchor[:, i] for i in range(4)]\n",
        "  else:\n",
        "    pos_labeled_anchor = pos_labeled_anchor.squeeze()\n",
        "    [pos_x_ctr,pos_y_ctr, pos_w, pos_h] = [pos_labeled_anchor[i] for i in range(4)]\n",
        "\n",
        "  gt_tx = (gt_x_ctr - pos_x_ctr) / pos_w\n",
        "  gt_ty = (gt_y_ctr - pos_y_ctr) / pos_h\n",
        "  gt_tw = torch.log(gt_w / pos_w)\n",
        "  gt_th = torch.log(gt_h / pos_h)\n",
        "\n",
        "  gt_reg = torch.stack([gt_tx, gt_ty, gt_tw, gt_th], dim = -1)\n",
        "  return gt_reg\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ9Atp0JHDMe",
        "colab_type": "text"
      },
      "source": [
        "##**Training RPN Network**##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne4TOlKp9AKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "import os \n",
        "import torch.nn as nn\n",
        "import time \n",
        "\n",
        "scales = torch.tensor([128, 256, 512])\n",
        "ratios = torch.tensor([0.5, 1, 2])\n",
        "\n",
        "# merge modules \n",
        "customResNet101 = nn.Sequential(model, MyConv5ofResnet101(), MyRPN())\n",
        "\n",
        "optimizer = optim.SGD(customResNet101.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "startTime = time.time()\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0\n",
        "\n",
        "  for i in range(4000):\n",
        "    image, x = get_input_img(i, 'train')    # 'val'\n",
        "\n",
        "    cls, reg = customResNet101(x) \n",
        "    anchor_xyxy, anchor_xywh = make_anchor_boxes(scales, ratios, image, cls)\n",
        "\n",
        "    pred_cls = cls.permute(0, 2, 3, 1).reshape(-1, 2)[:, 1]\n",
        "\n",
        "    gt_bboxes = image['img_gt_bboxes']\n",
        "    gt_xyxy = get_gt_xyxy(gt_bboxes)\n",
        "    IoU = get_IoU(gt_xyxy, anchor_xyxy)\n",
        "    label = make_label(IoU)\n",
        "\n",
        "    loss = nn.BCELoss()    \n",
        "    cls_loss = loss(pred_cls, label)    \n",
        "\n",
        "    _, gt_reg = get_gt_reg(gt_xyxy, anchor_xyxy)\n",
        "    pred_reg = reg.permute(0, 2, 3, 1).reshape(-1, 4)[label == 1].squeeze()\n",
        "\n",
        "    loss = nn.SmoothL1Loss()\n",
        "    reg_loss = loss(gt_reg, pred_reg)\n",
        "    total_loss = cls_loss / 256 + reg_loss * 10 / len(label)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += total_loss.item()\n",
        "    if i % 100 == 99:\n",
        "      print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
        "      running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_15u16t6GXC7",
        "colab_type": "text"
      },
      "source": [
        "##**Regression**##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJufMwxwZwvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# regressed anchor\n",
        "\n",
        "def regression(anchor, pred_reg):\n",
        "\n",
        "  anchor = anchor.squeeze()\n",
        "  [pos_x_ctr,pos_y_ctr, pos_w, pos_h] = [anchor[i] for i in range(4)]\n",
        "\n",
        "  pred_reg = pred_reg.squeeze()\n",
        "\n",
        "  rgrsd_x_ctr = pred_reg[0] * pos_w + pos_x_ctr\n",
        "  rgrsd_y_ctr = pred_reg[1] * pos_h + pos_y_ctr\n",
        "  rgrsd_w = torch.exp(pred_reg[2]) * pos_w\n",
        "  rgrsd_h = torch.exp(pred_reg[3]) * pos_h\n",
        "\n",
        "  rgrsd_anchor = torch.stack([rgrsd_x_ctr, rgrsd_y_ctr, rgrsd_w, rgrsd_h], dim = -1)\n",
        "  return rgrsd_anchor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAuj8xx0ERxj",
        "colab_type": "text"
      },
      "source": [
        "##**Inference**##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAySExZLHLlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "import os \n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow \n",
        "import numpy as np\n",
        "\n",
        "path = \"/content/data/test2017/\" \n",
        "\n",
        "file_list = os.listdir(path)\n",
        "file_list_jpg = [file for file in file_list if file.endswith('.jpg')]\n",
        "\n",
        "scales = torch.tensor([128, 256, 512])\n",
        "ratios = torch.tensor([0.5, 1, 2])\n",
        "\n",
        "i = 7   # random number  \n",
        "\n",
        "image, x = get_input_img(i, 'test')\n",
        "\n",
        "cls, reg = customResNet101(x) \n",
        "anchor_xyxy, anchor_xywh = make_anchor_boxes(scales, ratios, image, cls)\n",
        "\n",
        "pred_cls = cls.permute(0, 2, 3, 1).reshape(-1, 2)[:, 1]\n",
        "pred_reg = reg.permute(0, 2, 3, 1).reshape(-1, 4)\n",
        "\n",
        "anchor = anchor_xyxy[torch.argmax(pred_cls)]\n",
        "rgrsd_anchor = regression(anchor, pred_reg)\n",
        "\n",
        "if len(rgrsd_anchor) > 1:\n",
        "  rgrsd_coord = torch.stack([rgrsd_anchor[:, 0] - (rgrsd_anchor[:, 2] / 2), rgrsd_anchor[:, 1] - (rgrsd_anchor[:, 3] / 2), rgrsd_anchor[:, 0] + (rgrsd_anchor[:, 2] / 2), rgrsd_anchor[:, 1] + (rgrsd_anchor[:, 3] / 2)], dim = -1).int()\n",
        "\n",
        "img = x.cpu().reshape(224, 224, 3).numpy()\n",
        "\n",
        "for i in range(len(rgrsd_coord)):\n",
        "  cv2.rectangle(img, (rgrsd_coord[i, 0], rgrsd_coord[i, 1]), (rgrsd_coord[i, 2], rgrsd_coord[i, 3]), (0, 0, 255), 2)\n",
        "  cv2.putText(img, str(rgrsd_coord) + str(i), (rgrsd_coord[i, 0], rgrsd_coord[i, 1]), 1, 1, (0, 0, 255))\n",
        "\n",
        "cv2_imshow(img) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruoQME8_FPfT",
        "colab_type": "text"
      },
      "source": [
        "##**NMS**##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co8V95RwFSBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.ops import nms \n",
        "\n",
        "def nms(anchor_xyxy, pred_cls, label):\n",
        "  pos_labeled_anchor = anchor_xyxy[label == 1]\n",
        "  score = pred_cls[label == 1]\n",
        "  assert len(score) == len(pos_labeled_anchor)\n",
        "\n",
        "  idx = torch.nms(pos_labeled_anchor, score, 0.7)\n",
        "\n",
        "  proposals = pos_labeled_anchor[idx]\n",
        "  return proposals"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}